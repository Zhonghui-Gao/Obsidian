Introduction
在传统的教学场景中，


RELATED WORKS

YOLO(You Only Look Once)是一种经典的one-stage目标检测算法(YOLOv8结构图如图2所示)。而其中的YOLOv8是建立在其前面的历史版本的基础上迭代开发的的一种SOTA(state-of-the-art)模型。在YOLOv8中引入了新的功能和改进点，以进一步提升性能和灵活性，与之前的版本不同的是：加入了新的骨干网络，新的检测头，新的损失函数。YOLOv8的网络有三个部分组成：输入端(Input)，主干网络(Backbone)和头部网络（Head）。在真实的智慧教室复杂情况下，在检测过程中存在对象密集，对象相互遮挡等检测困难下还存在一些现实问题。


PConv

在人工智能的发展过程，越来越多的卷积神经网络被提出，其中最经典的为卷积神经网络(CNNs)，而对于卷积作用来说是为了更好的提取空间特征。但是在提取空间特征过程中追求低延迟和高吞吐量的快速是未来的方向。如何在有限的资源下，减少计算量而同时精度不会下降，减少整个神经网络参数量减少训练时间是很要必要的。而在FasterNet中的PConv提出可以只做部分卷积而不会影响精度，利用特征图上的冗余信息，只用其中的一部分进行卷积。PConv在部分输入通道上进行常规的卷积操作来进行空间特征的提取，而对于其余通道来说是保持不变的，从而减少计算量和内存访问的次数。
PConv的FLOPs可以定义如下：

在等式1中，h,w分别表示特征图的长和宽，c表示输入通道数，c_p则表示的是参与卷积通道

而对于常规的卷积来说，其卷积率为r=c_p/c=1/4，但是对于PConv来说其FLOPs只有常规的1/16
其内存访问量约为常规的1/4

在等式2中，

因此，本文利用这一优势，将YOLOv8在主干网络中的C2f模块相结合，在保证精度不会下降的前提下，优化网络的参数量和计数量。提出了C2f_PConv模块，提高其效率。


BIFormer中的Bi-Level Routing Attention (BRA)


Bi-Level Routing Attention (BRA)是一种在BIFormer中引入的新型注意力机制，为了解决MHSA的可扩展性问题，提升卷积神经网络中特征提取的效率和效果。BRA能够捕捉细粒度的局部细节以及更广泛的全局上下文，从而提升模型理解输入数据复杂模式的能力，将资源集中在最重要的输入部分，减少了不必要的计算，从而提高了计算效率，在不牺牲模型精度的情况下实现高效性，显著减少了相比传统注意力机制的FLOPs和M_AC，因此本文将该注意力机制(BRA)引入到YOLOv8教室检测模型当中。


Bi-Directional Feature Pyramid Network (BiFPN) 是一种改进的特征金字塔网络（Feature Pyramid Network, FPN），旨在增强卷积神经网络在目标检测和语义分割任务中的特征融合能力。传统的FPN仅使用自上而下的特征融合，而BiFPN则采用自上而下和自下而上的双向特征融合，同时引入特征特征融合方式进行融合。
因此，本文利用这一优势，加强对特征的融合进而提高目标检测的性能



ShapeIoU（Shape Intersection over Union）是一种评价几何形状相似性的新方法,该方法可以通过关注边界框本身的形状和尺度来计算损耗，从而使边界框回归更加准确。



本文提出的YOLOv8-PB2S模型的整体网络结构如图1所示，下面将分别介绍各模块的改进点。




为了提高检测精度，我们提出了一种改进的基于YOLOv8的课堂学生行为检测模型。在这项工作中，我们设计了一个新的目标检测框架，将 Res2Net 模块与 C2f 模块相结合，创建了 C2f_Res2block 模块。我们将原有 YOLOv8 中的所有 C2f 模块替换为该模块，并将跨空间学习的高效多尺度注意力模块引入颈部网络。增强型检测框架的整体结构如图 5 所示，说明了经典 YOLOv8 和改进的 YOLOv8 之间的差异。在图中，黄色背景框区域表示添加的 EMA 块，橙色背景框区域表示添加的多头注意力 （MHSA） 模块，绿色突出显示的方块表示替换的C2f_Res2block模块。实验结果验证了改进后的YOLOv8框架具有优异的性能和检测精度。



为了提高检测精度的同时进行轻量化处理，我们提出了一种改进的基于YOLOv8的学生课堂不同行为的检测模型。在这项工作中我们将C2f模块和PConv模块想结合，创建了C2f_PConv模块，同时将原来的YOLOv8网络中backbone网络和head网络所有的C2分模块都替换，同时在backbone网络引入BRA模块，在head部分引入BiFPN网络以及修改原先损失函数，引入新的损失函数ShapeIoU。图1和图2说明了原先的YOLOv8和改进之后的YOLOv8之间的差异所在。实验结果验证了本文提出的YOLOv8-PB2S网络框架具有优异的性能和检测精度，以及更少的计算量。


在YOLOv8网络中引入C2f模块当中，存在着Bottleneck模块，虽然串联的Bottleneck模块可以使得模型更好地融合和提取不同尺度的信息，但是同时

在YOLOv8网络中，C2f模块在backbone部分和在head部分中引入了不同层次的次数。而在该模块存在着Bottleneck模块，虽然串联的Bottleneck模块可以使得模型更好地融合和提取不同尺度的信息，但是同时也无异于增加了更大的计算量。Bottlenneck结构如图3所示。本来通过FasterNet中提出的网络结构，重新将原先的YOLOv8中C2f模块的Bottleneck进行改进，将FasterNet提出的PConv方法引入到新的Bottleneck中。C2f_PConv模块结构如图4所示。



改进的YOLOv8模型如图2所示，我们将BRA注意力机制引入到backbone网络的SPPF层之后，以更好地捕捉多尺度特征和上下文信息，使模型更好的处理复杂的数据。BRA注意力机制结构如图3所示。其工作原理如下：

首先，对于一个二维输入特征图 \( X \in \mathbb{R}^{H \times W \times C} \)，将其划分为 \( S \times S \) 个不重叠的区域，每个区域包含 \(\frac{HW}{S^2}\) 个特征向量。这个步骤通过将 \( X \) 重塑为 \( X_r \in \mathbb{R}^{S^2 \times \frac{HW}{S^2} \times C} \) 来实现。

其次，通过线性投影得到查询、键和值张量 \( Q, K, V \in \mathbb{R}^{S^2 \times \frac{HW}{S^2} \times C} \)：
\[
Q = X_r W_q, \quad K = X_r W_k, \quad V = X_r W_v,
\]
其中 \( W_q, W_k, W_v \in \mathbb{R}^{C \times C} \) 分别是查询、键和值的投影权重。
\( W_q, W_k, W_v \in \mathbb{R}^{C \times C} \) are the projection weights for the query, key, and value, respectively.

再次，此注意力机制通过构建有向图来找到每个区域需要关注的相关区域。具体来说，该机制首先通过对 \( Q \) 和 \( K \) 进行每区域平均操作得到区域级别的查询和键 \( Q_r, K_r \in \mathbb{R}^{S^2 \times C} \)，通过矩阵乘法计算区域间亲和图的邻接矩阵 \( A_r \in \mathbb{R}^{S^2 \times S^2} \)：
\[
A_r = Q_r (K_r)^T.
\]

邻接矩阵 \( A_r \) 中的条目表示两个区域在语义上的相关程度。通过保留每个区域最相关的 top-k 连接来修剪亲和图。换句话说，通过行级 top-k 操作得到路由索引矩阵 \( I_r \in \mathbb{N}^{S^2 \times k} \)：
\[
I_r = \text{topkIndex}(A_r).
\]
\( I_r \) 的第 \( i \) 行包含了与第 \( i \) 区域最相关的 k 个区域的索引。

再次，由于在\( A_r \)中的条目表示的是两个区域在语义上的相关程度，所以需要通过保留每个区域最相关的前k个区域得到裁剪后的\( A_r \)，将最不相关的给过滤掉，得到路由索引矩阵 \( I_r \in \mathbb{N}^{S^2 \times k} \)，



利用区域间路由索引矩阵 \( I_r \)，可以应用细粒度的 token 间注意力。对于区域 \( i \) 中的每个查询 token，它将关注位于通过 \( I_r(i,1), I_r(i,2), \ldots, I_r(i,k) \) 索引到的 k 个路由区域的所有键值对。

随后，将上述得到的区域间路由索引矩阵 \( I_r \)使用细粒度的 token 间注意力。对于区域 \( i \) 中的每个查询 token，仅仅关注位于通过 \( I_r(i,1), I_r(i,2), \ldots, I_r(i,k) \) 索引到的 k 个路由区域的所有键值对。从而收集这些在索引到的k个路由区域上所有K和V张量以获得收集键K_g和值张量V_g.

收集键和值张量：
\[
K_g = \text{gather}(K, I_r), \quad V_g = \text{gather}(V, I_r),
\]
其中 \( K_g, V_g \in \mathbb{R}^{S^2 \times \frac{kHW}{S^2} \times C} \) 是收集后的键和值张量。然后，可以在收集的键值对上应用注意力：
\[
O = \text{Attention}(Q, K_g, V_g) + \text{LCE}(V).
\]
通过这种方法，利用了稀疏性来跳过计算最不相关的区域，同时仅涉及对 GPU 友好的密集矩阵乘法。

最后，在收集后的键和值张量应用注意力同时再引入一个局部上下文增强项LCE，输出一个张量O




First, for a 2D input feature map \( X \in \mathbb{R}^{H \times W \times C} \), it is divided into \( S \times S \) non-overlapping regions, each containing \(\frac{HW}{S^2}\) feature vectors. This step is performed by reshaping \( X \) into \( X_r \in \mathbb{R}^{S^2 \times \frac{HW}{S^2} \times C} \).

Next, we obtain the query, key, and value tensors \( Q, K, V \in \mathbb{R}^{S^2 \times \frac{HW}{S^2} \times C} \) through linear projections:
\[
Q = X_r W_q, \quad K = X_r W_k, \quad V = X_r W_v,
\]
where \( W_q, W_k, W_v \in \mathbb{R}^{C \times C} \) are the projection weights for the query, key, and value, respectively.

Then, we find the related regions each region needs to attend to by constructing a directed graph. Specifically, we first obtain region-level queries and keys \( Q_r, K_r \in \mathbb{R}^{S^2 \times C} \) by applying per-region averaging on \( Q \) and \( K \), and compute the adjacency matrix \( A_r \in \mathbb{R}^{S^2 \times S^2} \) of the region affinity graph via matrix multiplication:
\[
A_r = Q_r (K_r)^T.
\]
Entries in the adjacency matrix \( A_r \) indicate the semantic relevance between two regions. We prune the affinity graph by retaining only the top-k connections for each region. Specifically, we derive the routing index matrix \( I_r \in \mathbb{N}^{S^2 \times k} \) with the row-wise top-k operation:
\[
I_r = \text{topkIndex}(A_r).
\]
The \( i \)-th row of \( I_r \) contains the indices of the k most relevant regions for the \( i \)-th region.

Using the region-to-region routing index matrix \( I_r \), we can apply fine-grained token-to-token attention. Each query token in region \( i \) attends to all key-value pairs in the k routed regions indexed by \( I_r(i,1), I_r(i,2), \ldots, I_r(i,k) \). However, implementing this step efficiently is challenging, as these routed regions might be scattered across the entire feature map, while modern GPUs rely on coalesced memory operations to load blocks of contiguous bytes.

We gather the key and value tensors:
\[
K_g = \text{gather}(K, I_r), \quad V_g = \text{gather}(V, I_r),
\]
where \( K_g, V_g \in \mathbb{R}^{S^2 \times \frac{kHW}{S^2} \times C} \) are the gathered key and value tensors. Attention is then applied to the gathered key-value pairs:
\[
O = \text{Attention}(Q, K_g, V_g) + \text{LCE}(V).
\]
This approach leverages sparsity to skip computations in the least relevant regions while involving only GPU-friendly dense matrix multiplications.



BiFPN
本文将BiFPN特征网络引入到YOLOv8中的Head部分，旨在能利用加权双向特征金字塔网络(BiFPN)高效的对多尺度特征融合，从而聚合不同分辨率的特征。对于传统的自下而上融合方式的FPN，以及有着自上而下和自下而上两个通道融合方式的PANet来说，BiFPN引入了加强特征融合机制，这使得网络可以学习到不同尺度特征的重要性，从而进行更有效的特征融合。原始的BiFPN结构如图3所示，引入YOLOv8中的BiFPN结构如图4所示。



Sure, here is the extracted text and formula from the provided image:

**Text:**
Fast normalized fusion: 

**Formula:**
\[ O = \sum_{i} \frac{w_i}{\epsilon + \sum_{j} w_j} \cdot I_i \]

**Text continued:**
where \( w_i \geq 0 \) is ensured by applying a ReLU after each \( w_i \), and \( \epsilon = 0.0001 \) is a small value to avoid numerical instability. Similarly, the value of each normalized weight also falls between 0 and 1, but since there is no softmax operation here, it is much more efficient. Our ablation study shows this fast fusion approach has very similar learning behavior and accuracy as the softmax-based fusion, but runs up to 30% faster on GPUs (Table 6).

在BiFPN网络中采取了快速归一化的带权特征融合方式，其中不同层的特征按照权重进行加权和，能够更好提高融合效果。



在上述的公式中\( O \)是融合后的输出特征，\( w_i \)是第 \( i \) 层特征的权重，为了保证该权重非负，在后级中应用 ReLU 函数，\( I_i \)是第 \( i \) 层的输入特征。\(\epsilon\)是一个小的常数值（通常为0.0001），用于避免数值不稳定。通过对每个输入特征 \( I_i \) 进行加权和，然后再对这些权重进行归一化，每个标准化权重的值都在0和1之间，从而实现特征融合。




- \( O \): 融合后的输出特征。
- \( w_i \): 第 \( i \) 层特征的权重。
- \( I_i \): 第 \( i \) 层的输入特征。
- \(\epsilon\): 一个小的常数值（通常为 0.0001），用于避免数值不稳定。

公式描述如下：

\[ O = \sum_i \frac{w_i}{\epsilon + \sum_j w_j} \cdot I_i \]

其中：

- \(\frac{w_i}{\epsilon + \sum_j w_j}\) 是第 \( i \) 层特征的归一化权重，保证权重 \( w_i \) 是非负的，具体是通过在每个 \( w_i \) 后面应用 ReLU 函数实现的。这个归一化权重在总和加上 \(\epsilon\) 后，使其在数值计算中更稳定。
- \( I_i \) 是第 \( i \) 层的输入特征。
- \(\epsilon\) 的作用是防止在权重总和非常小的时候，出现数值不稳定的情况。

总结：

公式通过对每个输入特征 \( I_i \) 进行加权和，然后再对这些权重进行归一化，使得所有权重的和为 1，从而实现特征融合。这种方法不仅提高了融合效果，而且避免了使用 softmax 操作，从而在 GPU 上运行速度更快。



ShapeIoU:

在损失函数计算方面，YOLOv8网络采用的C

Yolov8的回归损失计算分为CIou_Loss和Distribution Focal Loss(DFL)两部分，其中，CIou_Loss用于计算预测框与目标框之间的IoU。





在YOLOv8之前的YOLO版本中，边界框回归损失主要基于交并比（Intersection over Union, IoU）。以下是一些常用的IoU变体及其机制：

### 1. IoU（Intersection over Union）
标准的IoU度量用于评估预测框和真实框之间的重叠程度。它的公式为：

\[
\text{IoU} = \frac{\text{Area of Intersection}}{\text{Area of Union}}
\]

即：

\[
\text{IoU} = \frac{|B \cap B^{gt}|}{|B \cup B^{gp}|}
\]

其中 \( B = (x,y,w,h) \) 是预测边界框，\( B^{gt} = (x^{gt},y^{gt}, w^{gt}, h^{gt})\)是真实边界框。IoU loss其计算公式如下



### 4. CIoU（Complete Intersection over Union）
CIoU结合了DIoU并考虑了宽高比的影响，进一步提高了边界框的回归效果。CIoU 的公式为：

\[
\text{CIoU} = \text{IoU} - \left(\frac{\rho^2(B_p, B_t)}{c^2} + \alpha \cdot v \right)
\]

其中 \(v\) 是衡量宽高比一致性的参数，\(\alpha\) 是一个权重平衡参数。


YOLOv8在损失函数设计方面，回归损失计算分为CIou_Loss和Distribution Focal Loss(DFL)两部分。其中，CIou_Loss用于计算预测框与目标框之间的IoU。标准的IoU度量用于评估预测框和真实框之间的重叠程度。


而对于CIoU Loss其计算公式如下：



YOLOv8引入了一些改进和优化，使其在损失函数的设计上与之前的版本（如YOLOv5、YOLOv6）有所不同。这些改进主要体现在以下几个方面：

### 1. 改进的边界框回归损失（Bounding Box Regression Loss）
YOLOv8可能采用了更先进的IoU度量方式，例如CIoU（Complete IoU）或EIoU（Extended IoU），相比于之前的GIoU（Generalized IoU）或DIoU（Distance IoU），这些改进的IoU度量方式更好地处理了边界框的重叠和尺度差异问题，使得定位损失更为准确。

### 2. 动态权重平衡（Dynamic Weight Balancing）
为了更好地平衡不同损失项的权重，YOLOv8可能引入了动态权重平衡机制。传统的YOLO版本使用固定的权重超参数（\(\lambda_{\text{loc}}, \lambda_{\text{conf}}, \lambda_{\text{cls}}\)），而YOLOv8可能根据训练过程中的损失变化动态调整这些权重，从而在训练的不同阶段更好地平衡定位损失、置信度损失和分类损失。

### 3. 改进的置信度损失（Confidence Loss）
YOLOv8可能优化了置信度损失的计算方式，使其在处理正负样本时更加稳定。传统的YOLO版本使用二元交叉熵损失（Binary Cross-Entropy Loss），而YOLOv8可能通过引入Focal Loss等技术，增强对难检测目标的关注，减少易检测目标对损失的影响。

### 4. 更高效的特征融合（Feature Aggregation）
YOLOv8可能改进了特征融合方式，在Head部分引入了如BiFPN（Bi-directional Feature Pyramid Network）等更先进的特征融合网络，从而更好地利用不同尺度的特征进行目标检测。这不仅提升了模型的检测精度，也减少了计算复杂度。

### 5. 更稳定的训练过程
YOLOv8在训练过程中可能引入了一些新的正则化技术和优化策略，例如混合精度训练（Mixed Precision Training）、自动数据增强（AutoAugment）、超参数自动优化（AutoML）等，使得训练过程更为稳定，模型更具泛化能力。

### 损失函数公式改进示例

以下是YOLOv8损失函数的一个可能改进版本的数学公式：

\[
\mathcal{L} = \lambda_{\text{loc}} \mathcal{L}_{\text{loc}} + \lambda_{\text{conf}} \mathcal{L}_{\text{conf}} + \lambda_{\text{cls}} \mathcal{L}_{\text{cls}}
\]

其中：
- \(\mathcal{L}_{\text{loc}}\) 是改进后的定位损失，如CIoU损失：
  \[
  \mathcal{L}_{\text{loc}} = 1 - \text{CIoU}(B_p, B_t)
  \]
- \(\mathcal{L}_{\text{conf}}\) 是优化后的置信度损失，可能采用Focal Loss：
  \[
  \mathcal{L}_{\text{conf}} = - \sum_{i} \alpha (1 - p_i)^\gamma \log(p_i)
  \]
  其中 \(\alpha\) 和 \(\gamma\) 是Focal Loss的超参数，\(p_i\) 是置信度。
- \(\mathcal{L}_{\text{cls}}\) 是分类损失，仍然采用多元交叉熵损失：
  \[
  \mathcal{L}_{\text{cls}} = - \sum_{k} y_k \log(\hat{p}_k)
  \]

通过这些改进，YOLOv8在处理不同尺度、不同类别的目标检测任务时具有更高的准确性和效率。
