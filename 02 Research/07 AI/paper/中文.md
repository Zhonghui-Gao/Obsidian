Introduction
在传统的教学场景中，


RELATED WORKS

YOLO(You Only Look Once)是一种经典的one-stage目标检测算法(YOLOv8结构图如图2所示)。而其中的YOLOv8是建立在其前面的历史版本的基础上迭代开发的的一种SOTA(state-of-the-art)模型。在YOLOv8中引入了新的功能和改进点，以进一步提升性能和灵活性，与之前的版本不同的是：加入了新的骨干网络，新的检测头，新的损失函数。YOLOv8的网络有三个部分组成：输入端(Input)，主干网络(Backbone)和头部网络（Head）。在真实的智慧教室复杂情况下，在检测过程中存在对象密集，对象相互遮挡等检测困难下还存在一些现实问题。


PConv

在人工智能的发展过程，越来越多的卷积神经网络被提出，其中最经典的为卷积神经网络(CNNs)，而对于卷积作用来说是为了更好的提取空间特征。但是在提取空间特征过程中追求低延迟和高吞吐量的快速是未来的方向。如何在有限的资源下，减少计算量而同时精度不会下降，减少整个神经网络参数量减少训练时间是很要必要的。而在FasterNet中的PConv提出可以只做部分卷积而不会影响精度，利用特征图上的冗余信息，只用其中的一部分进行卷积。PConv在部分输入通道上进行常规的卷积操作来进行空间特征的提取，而对于其余通道来说是保持不变的，从而减少计算量和内存访问的次数。
PConv的FLOPs可以定义如下：

在等式1中，h,w分别表示特征图的长和宽，c表示输入通道数，c_p则表示的是参与卷积通道

而对于常规的卷积来说，其卷积率为r=c_p/c=1/4，但是对于PConv来说其FLOPs只有常规的1/16
其内存访问量约为常规的1/4

在等式2中，

因此，本文利用这一优势，将YOLOv8在主干网络中的C2f模块相结合，在保证精度不会下降的前提下，优化网络的参数量和计数量。提出了C2f_PConv模块，提高其效率。


BIFormer中的Bi-Level Routing Attention (BRA)


Bi-Level Routing Attention (BRA)是一种在BIFormer中引入的新型注意力机制，为了解决MHSA的可扩展性问题，提升卷积神经网络中特征提取的效率和效果。BRA能够捕捉细粒度的局部细节以及更广泛的全局上下文，从而提升模型理解输入数据复杂模式的能力，将资源集中在最重要的输入部分，减少了不必要的计算，从而提高了计算效率，在不牺牲模型精度的情况下实现高效性，显著减少了相比传统注意力机制的FLOPs和M_AC，因此本文将该注意力机制(BRA)引入到YOLOv8教室检测模型当中。


Bi-Directional Feature Pyramid Network (BiFPN) 是一种改进的特征金字塔网络（Feature Pyramid Network, FPN），旨在增强卷积神经网络在目标检测和语义分割任务中的特征融合能力。传统的FPN仅使用自上而下的特征融合，而BiFPN则采用自上而下和自下而上的双向特征融合，同时引入特征特征融合方式进行融合。
因此，本文利用这一优势，加强对特征的融合进而提高目标检测的性能



ShapeIoU（Shape Intersection over Union）是一种评价几何形状相似性的新方法,该方法可以通过关注边界框本身的形状和尺度来计算损耗，从而使边界框回归更加准确。



本文提出的YOLOv8-PB2S模型的整体网络结构如图1所示，下面将分别介绍各模块的改进点。




为了提高检测精度，我们提出了一种改进的基于YOLOv8的课堂学生行为检测模型。在这项工作中，我们设计了一个新的目标检测框架，将 Res2Net 模块与 C2f 模块相结合，创建了 C2f_Res2block 模块。我们将原有 YOLOv8 中的所有 C2f 模块替换为该模块，并将跨空间学习的高效多尺度注意力模块引入颈部网络。增强型检测框架的整体结构如图 5 所示，说明了经典 YOLOv8 和改进的 YOLOv8 之间的差异。在图中，黄色背景框区域表示添加的 EMA 块，橙色背景框区域表示添加的多头注意力 （MHSA） 模块，绿色突出显示的方块表示替换的C2f_Res2block模块。实验结果验证了改进后的YOLOv8框架具有优异的性能和检测精度。



为了提高检测精度的同时进行轻量化处理，我们提出了一种改进的基于YOLOv8的学生课堂不同行为的检测模型。在这项工作中我们将C2f模块和PConv模块想结合，创建了C2f_PConv模块，同时将原来的YOLOv8网络中backbone网络和head网络所有的C2分模块都替换，同时在backbone网络引入BRA模块，在head部分引入BiFPN网络以及修改原先损失函数，引入新的损失函数ShapeIoU。图1和图2说明了原先的YOLOv8和改进之后的YOLOv8之间的差异所在。实验结果验证了本文提出的YOLOv8-PB2S网络框架具有优异的性能和检测精度，以及更少的计算量。


在YOLOv8网络中引入C2f模块当中，存在着Bottleneck模块，虽然串联的Bottleneck模块可以使得模型更好地融合和提取不同尺度的信息，但是同时

在YOLOv8网络中，C2f模块在backbone部分和在head部分中引入了不同层次的次数。而在该模块存在着Bottleneck模块，虽然串联的Bottleneck模块可以使得模型更好地融合和提取不同尺度的信息，但是同时也无异于增加了更大的计算量。Bottlenneck结构如图3所示。本来通过FasterNet中提出的网络结构，重新将原先的YOLOv8中C2f模块的Bottleneck进行改进，将FasterNet提出的PConv方法引入到新的Bottleneck中。C2f_PConv模块结构如图4所示。



改进的YOLOv8模型如图2所示，我们将BRA注意力机制引入到backbone网络的SPPF层之后，以更好地捕捉多尺度特征和上下文信息，使模型更好的处理复杂的数据。BRA注意力机制结构如图3所示。其工作原理如下：

首先，对于一个二维输入特征图 \( X \in \mathbb{R}^{H \times W \times C} \)，将其划分为 \( S \times S \) 个不重叠的区域，每个区域包含 \(\frac{HW}{S^2}\) 个特征向量。这个步骤通过将 \( X \) 重塑为 \( X_r \in \mathbb{R}^{S^2 \times \frac{HW}{S^2} \times C} \) 来实现。

其次，通过线性投影得到查询、键和值张量 \( Q, K, V \in \mathbb{R}^{S^2 \times \frac{HW}{S^2} \times C} \)：
\[
Q = X_r W_q, \quad K = X_r W_k, \quad V = X_r W_v,
\]
其中 \( W_q, W_k, W_v \in \mathbb{R}^{C \times C} \) 分别是查询、键和值的投影权重。
\( W_q, W_k, W_v \in \mathbb{R}^{C \times C} \) are the projection weights for the query, key, and value, respectively.



再次，通过构建有向图来找到每个区域需要关注的相关区域。具体来说，首先通过对 \( Q \) 和 \( K \) 进行每区域平均操作得到区域级别的查询和键 \( Q_r, K_r \in \mathbb{R}^{S^2 \times C} \)，通过矩阵乘法计算区域间亲和图的邻接矩阵 \( A_r \in \mathbb{R}^{S^2 \times S^2} \)：
\[
A_r = Q_r (K_r)^T.
\]
邻接矩阵 \( A_r \) 中的条目表示两个区域在语义上的相关程度。通过保留每个区域最相关的 top-k 连接来修剪亲和图。换句话说，通过行级 top-k 操作得到路由索引矩阵 \( I_r \in \mathbb{N}^{S^2 \times k} \)：
\[
I_r = \text{topkIndex}(A_r).
\]
\( I_r \) 的第 \( i \) 行包含了与第 \( i \) 区域最相关的 k 个区域的索引。


利用区域间路由索引矩阵 \( I_r \)，可以应用细粒度的 token 间注意力。对于区域 \( i \) 中的每个查询 token，它将关注位于通过 \( I_r(i,1), I_r(i,2), \ldots, I_r(i,k) \) 索引到的 k 个路由区域的所有键值对。然而，实现这个步骤并非易事，因为这些路由区域可能分散在整个特征图中，而现代 GPU 依赖于加载连续字节块的联合内存操作。

收集键和值张量：
\[
K_g = \text{gather}(K, I_r), \quad V_g = \text{gather}(V, I_r),
\]
其中 \( K_g, V_g \in \mathbb{R}^{S^2 \times \frac{kHW}{S^2} \times C} \) 是收集后的键和值张量。然后，可以在收集的键值对上应用注意力：
\[
O = \text{Attention}(Q, K_g, V_g) + \text{LCE}(V).
\]
通过这种方法，利用了稀疏性来跳过计算最不相关的区域，同时仅涉及对 GPU 友好的密集矩阵乘法。



First, for a 2D input feature map \( X \in \mathbb{R}^{H \times W \times C} \), it is divided into \( S \times S \) non-overlapping regions, each containing \(\frac{HW}{S^2}\) feature vectors. This step is performed by reshaping \( X \) into \( X_r \in \mathbb{R}^{S^2 \times \frac{HW}{S^2} \times C} \).

Next, we obtain the query, key, and value tensors \( Q, K, V \in \mathbb{R}^{S^2 \times \frac{HW}{S^2} \times C} \) through linear projections:
\[
Q = X_r W_q, \quad K = X_r W_k, \quad V = X_r W_v,
\]
where \( W_q, W_k, W_v \in \mathbb{R}^{C \times C} \) are the projection weights for the query, key, and value, respectively.

Then, we find the related regions each region needs to attend to by constructing a directed graph. Specifically, we first obtain region-level queries and keys \( Q_r, K_r \in \mathbb{R}^{S^2 \times C} \) by applying per-region averaging on \( Q \) and \( K \), and compute the adjacency matrix \( A_r \in \mathbb{R}^{S^2 \times S^2} \) of the region affinity graph via matrix multiplication:
\[
A_r = Q_r (K_r)^T.
\]
Entries in the adjacency matrix \( A_r \) indicate the semantic relevance between two regions. We prune the affinity graph by retaining only the top-k connections for each region. Specifically, we derive the routing index matrix \( I_r \in \mathbb{N}^{S^2 \times k} \) with the row-wise top-k operation:
\[
I_r = \text{topkIndex}(A_r).
\]
The \( i \)-th row of \( I_r \) contains the indices of the k most relevant regions for the \( i \)-th region.

Using the region-to-region routing index matrix \( I_r \), we can apply fine-grained token-to-token attention. Each query token in region \( i \) attends to all key-value pairs in the k routed regions indexed by \( I_r(i,1), I_r(i,2), \ldots, I_r(i,k) \). However, implementing this step efficiently is challenging, as these routed regions might be scattered across the entire feature map, while modern GPUs rely on coalesced memory operations to load blocks of contiguous bytes.

We gather the key and value tensors:
\[
K_g = \text{gather}(K, I_r), \quad V_g = \text{gather}(V, I_r),
\]
where \( K_g, V_g \in \mathbb{R}^{S^2 \times \frac{kHW}{S^2} \times C} \) are the gathered key and value tensors. Attention is then applied to the gathered key-value pairs:
\[
O = \text{Attention}(Q, K_g, V_g) + \text{LCE}(V).
\]
This approach leverages sparsity to skip computations in the least relevant regions while involving only GPU-friendly dense matrix multiplications.