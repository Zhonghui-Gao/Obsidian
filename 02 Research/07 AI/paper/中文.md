Introduction
在传统的教学场景中，


RELATED WORKS
当前的目标检测深度学习算法是单阶段或者两阶段。而对于两阶段的典型算法有：R-CNN，Fast R-CNN， Faster R-CNN等等，而对于单阶段典型的算法有SSD和YOLO系列。
YOLO(You Only Look Once)是一种经典的one-stage目标检测算法(YOLOv8结构图如图2所示)。而其中的YOLOv8是建立在其前面的历史版本的基础上迭代开发的的一种SOTA(state-of-the-art)模型。在YOLOv8中引入了新的功能和改进点，以进一步提升性能和灵活性，与之前的版本不同的是：加入了新的骨干网络，新的检测头，新的损失函数。YOLOv8的网络有三个部分组成：输入端(Input)，主干网络(Backbone)和头部网络（Head）。在真实的智慧教室复杂情况下，在检测过程中存在对象密集，对象相互遮挡等检测困难下还存在一些现实问题。


PConv

在人工智能的发展过程，越来越多的卷积神经网络被提出，其中最经典的为卷积神经网络(CNNs)，而对于卷积作用来说是为了更好的提取空间特征。但是在提取空间特征过程中追求低延迟和高吞吐量的快速是未来的方向。如何在有限的资源下，减少计算量而同时精度不会下降，减少整个神经网络参数量减少训练时间是很要必要的。而在FasterNet中的PConv提出可以只做部分卷积而不会影响精度，利用特征图上的冗余信息，只用其中的一部分进行卷积。PConv在部分输入通道上进行常规的卷积操作来进行空间特征的提取，而对于其余通道来说是保持不变的，从而减少计算量和内存访问的次数。
PConv的FLOPs可以定义如下：

在等式1中，h,w分别表示特征图的长和宽，c表示输入通道数，c_p则表示的是参与卷积通道

而对于常规的卷积来说，其卷积率为r=c_p/c=1/4，但是对于PConv来说其FLOPs只有常规的1/16
其内存访问量约为常规的1/4

在等式2中，

因此，本文利用这一优势，将YOLOv8在主干网络中的C2f模块相结合，在保证精度不会下降的前提下，优化网络的参数量和计数量。提出了C2f_PConv模块，提高其效率。


BIFormer中的Bi-Level Routing Attention (BRA)


Bi-Level Routing Attention (BRA)是一种在BIFormer中引入的新型注意力机制，为了解决MHSA的可扩展性问题，提升卷积神经网络中特征提取的效率和效果。BRA能够捕捉细粒度的局部细节以及更广泛的全局上下文，从而提升模型理解输入数据复杂模式的能力，将资源集中在最重要的输入部分，减少了不必要的计算，从而提高了计算效率，在不牺牲模型精度的情况下实现高效性，显著减少了相比传统注意力机制的FLOPs和M_AC，因此本文将该注意力机制(BRA)引入到YOLOv8教室检测模型当中。


Efficient Multi-Scale Attention Module(EMA)
Efficient Multi-Scale Attention Module(EMA)是一种专为计算机视觉任务设计的注意力机制，能够保留每个通道的关键信息同时，减少计算开销，提升卷积神经网络中特征提取的效率和效果，在不牺牲模型精度的情况下实现高效性，减少了相比传统注意力机制的FLOPs，因此本文将该注意力机制(EMA)引入到YOLOv8教室检测模型当中。


Bi-Directional Feature Pyramid Network (BiFPN) 是一种改进的特征金字塔网络（Feature Pyramid Network, FPN），旨在增强卷积神经网络在目标检测和语义分割任务中的特征融合能力。传统的FPN仅使用自上而下的特征融合，而BiFPN则采用自上而下和自下而上的双向特征融合，同时引入特征特征融合方式进行融合。
因此，本文利用这一优势，加强对特征的融合进而提高目标检测的性能



ShapeIoU（Shape Intersection over Union）是一种评价几何形状相似性的新方法,该方法可以通过关注边界框本身的形状和尺度来计算损耗，从而使边界框回归更加准确。



本文提出的YOLOv8-PB2S模型的整体网络结构如图1所示，下面将分别介绍各模块的改进点。




为了提高检测精度，我们提出了一种改进的基于YOLOv8的课堂学生行为检测模型。在这项工作中，我们设计了一个新的目标检测框架，将 Res2Net 模块与 C2f 模块相结合，创建了 C2f_Res2block 模块。我们将原有 YOLOv8 中的所有 C2f 模块替换为该模块，并将跨空间学习的高效多尺度注意力模块引入颈部网络。增强型检测框架的整体结构如图 5 所示，说明了经典 YOLOv8 和改进的 YOLOv8 之间的差异。在图中，黄色背景框区域表示添加的 EMA 块，橙色背景框区域表示添加的多头注意力 （MHSA） 模块，绿色突出显示的方块表示替换的C2f_Res2block模块。实验结果验证了改进后的YOLOv8框架具有优异的性能和检测精度。



为了提高检测精度的同时进行轻量化处理，我们提出了一种改进的基于YOLOv8的学生课堂不同行为的检测模型。在这项工作中我们将C2f模块和PConv模块想结合，创建了C2f_PConv模块，同时将原来的YOLOv8网络中backbone网络和head网络所有的C2分模块都替换，同时在backbone网络引入BRA模块，在head部分引入BiFPN网络以及修改原先损失函数，引入新的损失函数ShapeIoU。图1和图2说明了原先的YOLOv8和改进之后的YOLOv8之间的差异所在。实验结果验证了本文提出的YOLOv8-PB2S网络框架具有优异的性能和检测精度，以及更少的计算量。


在YOLOv8网络中引入C2f模块当中，存在着Bottleneck模块，虽然串联的Bottleneck模块可以使得模型更好地融合和提取不同尺度的信息，但是同时

在YOLOv8网络中，C2f模块在backbone部分和在head部分中引入了不同层次的次数。而在该模块存在着Bottleneck模块，虽然串联的Bottleneck模块可以使得模型更好地融合和提取不同尺度的信息，但是同时也无异于增加了更大的计算量。Bottlenneck结构如图3所示。本来通过FasterNet中提出的网络结构，重新将原先的YOLOv8中C2f模块的Bottleneck进行改进，将FasterNet提出的PConv方法和EMA注意力机制引入到新的Bottleneck中。C2f_PConv模块结构如图4所示。


改进的YOLOv8模型如图2所示，我们将EMA注意力机制引入到Bottleneck，已更好的聚焦于特征图最具信息量的部分，并行处理特征，使模型更好的处理复杂的数据。EMA注意力机制结构如图3所示。EMA采用了并行路径来提取分组特征映射，为了能够对齐上述3x3的部分卷积部分，这里分别采用2路的1x1卷积分支和1路3x3卷积分支。在两路1x1分支中的并行实现不同跨通道的交互特征，而1路的3x3卷积用来扩大特征空间。在这里我们引入两个张量，其中一个是1x1分支的输出，另一个是3x3分支的输出，然后在1x1分支中输出全局空间信息，利用二维全局平均池把最小分支的输出直接转换成为相应尺寸的形状。The formula for the 2D global pooling operation is


其工作原理如下：





改进的YOLOv8模型如图2所示，我们将BRA注意力机制引入到backbone网络的SPPF层之后，以更好地捕捉多尺度特征和上下文信息，使模型更好的处理复杂的数据。BRA注意力机制结构如图3所示。其工作原理如下：

首先，对于一个二维输入特征图 \( X \in \mathbb{R}^{H \times W \times C} \)，将其划分为 \( S \times S \) 个不重叠的区域，每个区域包含 \(\frac{HW}{S^2}\) 个特征向量。这个步骤通过将 \( X \) 重塑为 \( X_r \in \mathbb{R}^{S^2 \times \frac{HW}{S^2} \times C} \) 来实现。

其次，通过线性投影得到查询、键和值张量 \( Q, K, V \in \mathbb{R}^{S^2 \times \frac{HW}{S^2} \times C} \)：
\[
Q = X_r W_q, \quad K = X_r W_k, \quad V = X_r W_v,
\]
其中 \( W_q, W_k, W_v \in \mathbb{R}^{C \times C} \) 分别是查询、键和值的投影权重。
\( W_q, W_k, W_v \in \mathbb{R}^{C \times C} \) are the projection weights for the query, key, and value, respectively.

再次，此注意力机制通过构建有向图来找到每个区域需要关注的相关区域。具体来说，该机制首先通过对 \( Q \) 和 \( K \) 进行每区域平均操作得到区域级别的查询和键 \( Q_r, K_r \in \mathbb{R}^{S^2 \times C} \)，通过矩阵乘法计算区域间亲和图的邻接矩阵 \( A_r \in \mathbb{R}^{S^2 \times S^2} \)：
\[
A_r = Q_r (K_r)^T.
\]

邻接矩阵 \( A_r \) 中的条目表示两个区域在语义上的相关程度。通过保留每个区域最相关的 top-k 连接来修剪亲和图。换句话说，通过行级 top-k 操作得到路由索引矩阵 \( I_r \in \mathbb{N}^{S^2 \times k} \)：
\[
I_r = \text{topkIndex}(A_r).
\]
\( I_r \) 的第 \( i \) 行包含了与第 \( i \) 区域最相关的 k 个区域的索引。

再次，由于在\( A_r \)中的条目表示的是两个区域在语义上的相关程度，所以需要通过保留每个区域最相关的前k个区域得到裁剪后的\( A_r \)，将最不相关的给过滤掉，得到路由索引矩阵 \( I_r \in \mathbb{N}^{S^2 \times k} \)，



利用区域间路由索引矩阵 \( I_r \)，可以应用细粒度的 token 间注意力。对于区域 \( i \) 中的每个查询 token，它将关注位于通过 \( I_r(i,1), I_r(i,2), \ldots, I_r(i,k) \) 索引到的 k 个路由区域的所有键值对。

随后，将上述得到的区域间路由索引矩阵 \( I_r \)使用细粒度的 token 间注意力。对于区域 \( i \) 中的每个查询 token，仅仅关注位于通过 \( I_r(i,1), I_r(i,2), \ldots, I_r(i,k) \) 索引到的 k 个路由区域的所有键值对。从而收集这些在索引到的k个路由区域上所有K和V张量以获得收集键K_g和值张量V_g.

收集键和值张量：
\[
K_g = \text{gather}(K, I_r), \quad V_g = \text{gather}(V, I_r),
\]
其中 \( K_g, V_g \in \mathbb{R}^{S^2 \times \frac{kHW}{S^2} \times C} \) 是收集后的键和值张量。然后，可以在收集的键值对上应用注意力：
\[
O = \text{Attention}(Q, K_g, V_g) + \text{LCE}(V).
\]
通过这种方法，利用了稀疏性来跳过计算最不相关的区域，同时仅涉及对 GPU 友好的密集矩阵乘法。

最后，在收集后的键和值张量应用注意力同时再引入一个局部上下文增强项LCE，输出一个张量O




First, for a 2D input feature map \( X \in \mathbb{R}^{H \times W \times C} \), it is divided into \( S \times S \) non-overlapping regions, each containing \(\frac{HW}{S^2}\) feature vectors. This step is performed by reshaping \( X \) into \( X_r \in \mathbb{R}^{S^2 \times \frac{HW}{S^2} \times C} \).

Next, we obtain the query, key, and value tensors \( Q, K, V \in \mathbb{R}^{S^2 \times \frac{HW}{S^2} \times C} \) through linear projections:
\[
Q = X_r W_q, \quad K = X_r W_k, \quad V = X_r W_v,
\]
where \( W_q, W_k, W_v \in \mathbb{R}^{C \times C} \) are the projection weights for the query, key, and value, respectively.

Then, we find the related regions each region needs to attend to by constructing a directed graph. Specifically, we first obtain region-level queries and keys \( Q_r, K_r \in \mathbb{R}^{S^2 \times C} \) by applying per-region averaging on \( Q \) and \( K \), and compute the adjacency matrix \( A_r \in \mathbb{R}^{S^2 \times S^2} \) of the region affinity graph via matrix multiplication:
\[
A_r = Q_r (K_r)^T.
\]
Entries in the adjacency matrix \( A_r \) indicate the semantic relevance between two regions. We prune the affinity graph by retaining only the top-k connections for each region. Specifically, we derive the routing index matrix \( I_r \in \mathbb{N}^{S^2 \times k} \) with the row-wise top-k operation:
\[
I_r = \text{topkIndex}(A_r).
\]
The \( i \)-th row of \( I_r \) contains the indices of the k most relevant regions for the \( i \)-th region.

Using the region-to-region routing index matrix \( I_r \), we can apply fine-grained token-to-token attention. Each query token in region \( i \) attends to all key-value pairs in the k routed regions indexed by \( I_r(i,1), I_r(i,2), \ldots, I_r(i,k) \). However, implementing this step efficiently is challenging, as these routed regions might be scattered across the entire feature map, while modern GPUs rely on coalesced memory operations to load blocks of contiguous bytes.

We gather the key and value tensors:
\[
K_g = \text{gather}(K, I_r), \quad V_g = \text{gather}(V, I_r),
\]
where \( K_g, V_g \in \mathbb{R}^{S^2 \times \frac{kHW}{S^2} \times C} \) are the gathered key and value tensors. Attention is then applied to the gathered key-value pairs:
\[
O = \text{Attention}(Q, K_g, V_g) + \text{LCE}(V).
\]
This approach leverages sparsity to skip computations in the least relevant regions while involving only GPU-friendly dense matrix multiplications.



BiFPN
本文将BiFPN特征网络引入到YOLOv8中的Head部分，旨在能利用加权双向特征金字塔网络(BiFPN)高效的对多尺度特征融合，从而聚合不同分辨率的特征。对于传统的自下而上融合方式的FPN，以及有着自上而下和自下而上两个通道融合方式的PANet来说，BiFPN引入了加强特征融合机制，这使得网络可以学习到不同尺度特征的重要性，从而进行更有效的特征融合。原始的BiFPN结构如图3所示，引入YOLOv8中的BiFPN结构如图4所示。



Sure, here is the extracted text and formula from the provided image:

**Text:**
Fast normalized fusion: 

**Formula:**
\[ O = \sum_{i} \frac{w_i}{\epsilon + \sum_{j} w_j} \cdot I_i \]

**Text continued:**
where \( w_i \geq 0 \) is ensured by applying a ReLU after each \( w_i \), and \( \epsilon = 0.0001 \) is a small value to avoid numerical instability. Similarly, the value of each normalized weight also falls between 0 and 1, but since there is no softmax operation here, it is much more efficient. Our ablation study shows this fast fusion approach has very similar learning behavior and accuracy as the softmax-based fusion, but runs up to 30% faster on GPUs (Table 6).

在BiFPN网络中采取了快速归一化的带权特征融合方式，其中不同层的特征按照权重进行加权和，能够更好提高融合效果。



在上述的公式中\( O \)是融合后的输出特征，\( w_i \)是第 \( i \) 层特征的权重，为了保证该权重非负，在后级中应用 ReLU 函数，\( I_i \)是第 \( i \) 层的输入特征。\(\epsilon\)是一个小的常数值（通常为0.0001），用于避免数值不稳定。通过对每个输入特征 \( I_i \) 进行加权和，然后再对这些权重进行归一化，每个标准化权重的值都在0和1之间，从而实现特征融合。




- \( O \): 融合后的输出特征。
- \( w_i \): 第 \( i \) 层特征的权重。
- \( I_i \): 第 \( i \) 层的输入特征。
- \(\epsilon\): 一个小的常数值（通常为 0.0001），用于避免数值不稳定。

公式描述如下：

\[ O = \sum_i \frac{w_i}{\epsilon + \sum_j w_j} \cdot I_i \]

其中：

- \(\frac{w_i}{\epsilon + \sum_j w_j}\) 是第 \( i \) 层特征的归一化权重，保证权重 \( w_i \) 是非负的，具体是通过在每个 \( w_i \) 后面应用 ReLU 函数实现的。这个归一化权重在总和加上 \(\epsilon\) 后，使其在数值计算中更稳定。
- \( I_i \) 是第 \( i \) 层的输入特征。
- \(\epsilon\) 的作用是防止在权重总和非常小的时候，出现数值不稳定的情况。

总结：

公式通过对每个输入特征 \( I_i \) 进行加权和，然后再对这些权重进行归一化，使得所有权重的和为 1，从而实现特征融合。这种方法不仅提高了融合效果，而且避免了使用 softmax 操作，从而在 GPU 上运行速度更快。



ShapeIoU:

在损失函数计算方面，YOLOv8网络采用的C

Yolov8的回归损失计算分为CIou_Loss和Distribution Focal Loss(DFL)两部分，其中，CIou_Loss用于计算预测框与目标框之间的IoU。





在YOLOv8之前的YOLO版本中，边界框回归损失主要基于交并比（Intersection over Union, IoU）。以下是一些常用的IoU变体及其机制：

### 1. IoU（Intersection over Union）
标准的IoU度量用于评估预测框和真实框之间的重叠程度。它的公式为：

\[
\text{IoU} = \frac{\text{Area of Intersection}}{\text{Area of Union}}
\]

即：

\[
\text{IoU} = \frac{|B \cap B^{gt}|}{|B \cup B^{gp}|}
\]

其中 \( B = (x,y,w,h) \) 是预测边界框，\( B^{gt} = (x^{gt},y^{gt}, w^{gt}, h^{gt})\)是真实边界框。IoU loss其计算公式如下



### 4. CIoU（Complete Intersection over Union）
CIoU结合了DIoU并考虑了宽高比的影响，进一步提高了边界框的回归效果。CIoU 的公式为：

\[
\text{CIoU} = \text{IoU} - \left(\frac{\rho^2(B_p, B_t)}{c^2} + \alpha \cdot v \right)
\]

其中 \(v\) 是衡量宽高比一致性的参数，\(\alpha\) 是一个权重平衡参数。


YOLOv8在损失函数设计方面，回归损失计算分为CIou_Loss和Distribution Focal Loss(DFL)两部分。其中，CIou_Loss用于计算预测框与目标框之间的IoU。标准的IoU度量用于评估预测框和真实框之间的重叠程度。


而对于CIoU Loss其计算公式如下：



YOLOv8引入了一些改进和优化，使其在损失函数的设计上与之前的版本（如YOLOv5、YOLOv6）有所不同。这些改进主要体现在以下几个方面：

### 1. 改进的边界框回归损失（Bounding Box Regression Loss）
YOLOv8可能采用了更先进的IoU度量方式，例如CIoU（Complete IoU）或EIoU（Extended IoU），相比于之前的GIoU（Generalized IoU）或DIoU（Distance IoU），这些改进的IoU度量方式更好地处理了边界框的重叠和尺度差异问题，使得定位损失更为准确。

### 2. 动态权重平衡（Dynamic Weight Balancing）
为了更好地平衡不同损失项的权重，YOLOv8可能引入了动态权重平衡机制。传统的YOLO版本使用固定的权重超参数（\(\lambda_{\text{loc}}, \lambda_{\text{conf}}, \lambda_{\text{cls}}\)），而YOLOv8可能根据训练过程中的损失变化动态调整这些权重，从而在训练的不同阶段更好地平衡定位损失、置信度损失和分类损失。

### 3. 改进的置信度损失（Confidence Loss）
YOLOv8可能优化了置信度损失的计算方式，使其在处理正负样本时更加稳定。传统的YOLO版本使用二元交叉熵损失（Binary Cross-Entropy Loss），而YOLOv8可能通过引入Focal Loss等技术，增强对难检测目标的关注，减少易检测目标对损失的影响。

### 4. 更高效的特征融合（Feature Aggregation）
YOLOv8可能改进了特征融合方式，在Head部分引入了如BiFPN（Bi-directional Feature Pyramid Network）等更先进的特征融合网络，从而更好地利用不同尺度的特征进行目标检测。这不仅提升了模型的检测精度，也减少了计算复杂度。

### 5. 更稳定的训练过程
YOLOv8在训练过程中可能引入了一些新的正则化技术和优化策略，例如混合精度训练（Mixed Precision Training）、自动数据增强（AutoAugment）、超参数自动优化（AutoML）等，使得训练过程更为稳定，模型更具泛化能力。

### 损失函数公式改进示例

以下是YOLOv8损失函数的一个可能改进版本的数学公式：

\[
\mathcal{L} = \lambda_{\text{loc}} \mathcal{L}_{\text{loc}} + \lambda_{\text{conf}} \mathcal{L}_{\text{conf}} + \lambda_{\text{cls}} \mathcal{L}_{\text{cls}}
\]

其中：
- \(\mathcal{L}_{\text{loc}}\) 是改进后的定位损失，如CIoU损失：
  \[
  \mathcal{L}_{\text{loc}} = 1 - \text{CIoU}(B_p, B_t)
  \]
- \(\mathcal{L}_{\text{conf}}\) 是优化后的置信度损失，可能采用Focal Loss：
  \[
  \mathcal{L}_{\text{conf}} = - \sum_{i} \alpha (1 - p_i)^\gamma \log(p_i)
  \]
  其中 \(\alpha\) 和 \(\gamma\) 是Focal Loss的超参数，\(p_i\) 是置信度。
- \(\mathcal{L}_{\text{cls}}\) 是分类损失，仍然采用多元交叉熵损失：
  \[
  \mathcal{L}_{\text{cls}} = - \sum_{k} y_k \log(\hat{p}_k)
  \]

通过这些改进，YOLOv8在处理不同尺度、不同类别的目标检测任务时具有更高的准确性和效率。


\begin{equation}
    \mathcal{L}_{\text{IoU}} = 1 - \frac{|B \cap B^{gt}|}{|B \cup B^{gt}|}
\end{equation}

而YOLOv8中采用的损失函数CIoU其计算公式如公式9所示
\[
\text{CIoU} = \text{IoU} - \left(\frac{\rho^2(B_p, B_t)}{c^2} + \alpha \cdot v \right)
\]

其中b和bgt表示两个矩形框的中心点，ρ表示两个矩形框之间的欧式距离，c表示两个矩形框闭合区域的对角线距离，v用于测量两个矩形框相对比例的一致性，α是权重系数

本文选择Shape-IoU替换原来的损失函数，通过关注边界框本身的形状和尺度来计算损耗，从而使边界框回归更加准确。其损失函数公式如公式9所示：

\


Exp:
本文的研究实验训练的硬件设备为Intel(R) Core(TM) i5-13490F@2.50 GHz CPU和NVIDIA GeForce RTX 4060 8G。软件环境为win11系统，64位，Python版本为3.8,0，Pytorch版本为2.3.0+cu121。
下面是一些参数设置用于实验。训练在YOLOv8n模型基础上进行训练，在训练过程中优化器选择自动选择，选择的优化器为SGD，学习率为0.01，动量选择为0.9，权重衰减系数为0.0005。同时规定输入图像大小为3通道图像的640×640像素，每个训练batch大小为16，训练轮数为500。训练轮数在一定周期之后如果当平均精度没有明显提升就会提前截停训练，将生成训练的结果。而其他的训练参数均设置为默认。

DataSet：
在本文工作中，我们使用公共数据集——学生课堂行为数据集(SCB-Dataset)。该数据集中包含着三种学生的课堂行为，包括举手、阅读和书写。SCB-Dateset包括4.2k张图像和18.4k标签。图6显示了每个行为的数量和一些数据集细节。图7给出了 SCB-Dataset中每个类别的实际示例。
![[Pasted image 20240613104846.png]]

图6所示。SeaShips数据集的详细信息;(a)SCB-Dataset数据集中每个类别的实例数量;(b)实例的GT框的大小;(c)GT框中点的坐标;(d)GT框的高度和宽度。

数据集中三种行为的示例；


指标：
评估指标对于模型性能的评估是非常重要的。为了评估模型的性能，本文以精度、召回率、FLOP（浮点运算）、mAP（平均精度）、模型大小和 FPS（每秒帧数）作为算法性能的评价指标，分别表示模型对目标类别和位置的预测准确率、正确预测目标占所有预测目标的比例、正确预测目标占所有真实目标的比例。精确率和召回率通过公式（8）和（9）计算。公式如下所示：

其中TP是真阳性，FP是假阳性，FN是假阴性



平均精度 (AP)：AP 计算精度-召回曲线下的面积，提供一个包含模型精度和召回性能的单一值。


平均精度 (AP) 通常用以下公式计算：

\[ \text{AP} = \int_0^1 P(R) \, dR \]

其中 \( P(R) \) 是在不同召回率 \( R \) 下的精度 \( P \)。

在实际计算中，通常使用离散的计算方式，将召回率分段，并计算每一段的精度：

\[ \text{AP} = \sum_{n} \left( R_n - R_{n-1} \right) P_n \]

其中 \( R_n \) 和 \( R_{n-1} \) 是相邻的召回率值， \( P_n \) 是对应的精度值。


平均平均精度(mAP):它通过计算所有类别的平均精度（AP）来衡量模型的整体性能。
mAP（mean Average Precision，平均平均精度）是一种用于评估目标检测和图像分类模型性能的指标。它通过计算所有类别的平均精度（AP）来衡量模型的整体性能。具体来说，mAP 是针对每个类别计算 AP，然后对这些 AP 求平均值。AP 是通过计算精度-召回曲线下的面积（AUC）得到的。

mAP 的计算步骤如下：

1. **计算每个类别的平均精度（AP）：**
    - 对于每个类别，绘制精度-召回曲线。
    - 计算精度-召回曲线下的面积，这就是该类别的 AP。

2. **计算所有类别的 mAP：**
    - 将所有类别的 AP 相加，并除以类别的总数，得到 mAP。

在目标检测任务中，通常会根据不同的 IoU（Intersection over Union）阈值来计算 mAP。例如，常见的 mAP@0.5 表示 IoU 阈值为 0.5 时的 mAP，而 mAP@0.5:0.95 则表示在不同 IoU 阈值（从 0.5 到 0.95，每隔 0.05 一个阈值）的平均 mAP。

公式如下所示：

\[
\text{mAP} = \frac{1}{C} \sum_{c=1}^{C} \text{AP}_c
\]

其中,\( C \) 是类别的总数 \(\text{AP}_c\) 是第 \(c\) 类的平均精度



为了证明改进模型的检测性能提升效果，我们进行了改进模型与基线模型YOLOv8s的对比实验。如表1和表2的比较结果所示，改进模型的mAP值几乎保持不变。但是在准确率上相比原模型提高1.5%，模型大小减少46.7%，参数量减少55%，GFlops减少36%，表明改进后的模型可以在运算量和模型大小情况下有效提高教室环境下学生行为的检测精度，提高检测性能，为实时检测做出了有效的提升。

总结：
我们对SCB-数据集和 CrowdHuman 数据集进行了实验，结果表明，目标检测精度有了显著的提高，其中mAP@0.5增加了4.2%和2.1%，分别比原来的YOLOv8模型。我们建议的学生课堂行为框架解决了课堂视频图像中常见的挑战如密度，相互遮挡，和多尺度的场景。为了应对这些挑战，我们推出了一系列创新方法。首先，我们在 YOLOv8中集成了 Res2Net 模块和原来的C2f模块，创建了一个新的C2f Res2block 模块。该模块有效地处理多尺度场景，同时提高了模型精度。此外，我们还引入了高效的多尺度注意模块(EMA)和多头自我注意(MHSA)机制模块，进一步增强了型的性能。未来，我们将继续专注干通过图像增强技术提高精确度，降低网络参数，并应对低质量原始视频图像的挑战。这些增强将有助于进一步完善我们的学生课堂行为检测框架


我们在课程学生行为检测的复杂情况下如相互遮挡，学生密度情况下下，基于SCB-Dataset下训练提出一种轻量化的模型，首先在YOLOv8中骨干网络中集成了C2f_PConv_EMA模块替换原先的C2f模块，在轻量化的同时保持精度不降低，此外，我们还在头部网络替换原先的特征融合网络PANet换成BiFPN，也旨在加强模型特征融合能力，同时减少模型的权重。结果表明，我们在保持精度不变的前提下，准确率上相比原模型提高1.5%，模型大小减少46.7%，参数量减少55%，GFlops减少36%。未来，我们将继续专注轻量化部分，然后将模型部署到边缘设备上，实现课堂质量的反馈和检测。


行为检测技术[1]使分析课堂视频中学生的行为成为可能，它可以提供有关学生课堂状态和学习表现的信息，使其成为学校教师、管理人员、学生和家长的必备工具。然而，在传统的教学方法中，教师很难观察每个学生的学习情况，学校管理人员通常依靠现场观察和学生表现报告来识别教育问题，而家长则依靠与教师和学生的沟通来了解孩子的学习情况。因此，利用行为检测技术准确检测和分析学生行为，可以为教育教学提供全面准确的反馈。现有的学生行为检测算法可分为三类：基于视频动作识别的[2]、基于姿态估计的[3]和基于物体检测的[4]，后者由于最近的突破而成为一种很有前途的解决方案。尽管基于视频的检测允许识别连续行为，但它需要大量带注释的样本，例如在 AVA 数据集 [5] 中用于 SlowFast [6] 检测，其中包括 158 万个注释。在此处确定适用的资助机构。如果没有，请删除它。视频行为识别仍在开发中，在某些情况下，动作只能仅通过上下文或场景来确定，如UCF101 [7]和Kinetics400 [8]所示。姿势估计算法可以获取关节位置和运动信息，但不足以检测过度拥挤的教室中的行为。基于眼前的挑战，本文采用了一种基于对象检测的方法，如YOLOv7 [9]来分析学生的行为。目标检测网络在公共数据集上显示出令人印象深刻的结果[10]，[11]。然而，课堂行为检测存在一些挑战，例如在不同环境、不同人与不同角度之间发生的行为变化，以及与MS COCO等流行的对象检测数据集相比，学生被遮挡[12]。在这项研究中，我们研究了对象检测在自动识别课堂上重要学生行为（例如举手、阅读和写作）方面的潜力。为此，我们构建了学生课堂行为数据集（SCB-Dataset）。SCB数据集填补了当前在课堂教学场景中检测学生行为的研究空白。我们进行了大量的数据统计和基准测试，以确保数据集的质量，提供可靠的训练数据。在这项研究中，我们研究了对象检测网络在自动识别课堂上重要学生行为的潜力，例如举手、阅读和写作。为此，我们构建了学生课堂行为数据集（SCB-Dataset）。SCB数据集填补了当前在课堂教学场景中检测学生行为的研究空白。我们进行了大量的数据统计和基准测试，以确保数据集的质量，提供可靠的训练数据。此外，我们没有在数据集中包括站立、坐着和说话的行为，因为它们已经包含在 AVA 数据集中，我们可以直接使用。YOLOv7 是目前可用的最好的单阶段目标检测算法之一，我们试图在 SCB-Dataset 上训练它以获得更好的检测结果。但是，我们 arXiv：2306.03318v1 [cs.CV] 2023 年 6 月 6 日 图 1.基于改进YOLOv7的学生课堂行为检测框架。发现 YOLOv7 的原始版本在训练后仍有一些改进空间——例如，它会将其他动作误认为举手，无法检测到较小的举手动作。此外，在学生场景密集或学生之间遮挡的区域，算法生成的边界框经常会显示一些错误。为了解决这些问题，我们采用了两种新方法，如图1所示。首先，我们合并了一个动态稀疏注意力模块，称为双层路由注意力[13]（BRA）。此外，我们采用了动态非单调调频使用异常值度，称为Wise-IoU[14]，成功地提高了检测性能。我们的主要贡献如下：（1）本文构建了学生在课堂上举手的数据集，从而更好地研究了课堂行为检测。此外，SCB数据集填补了教育领域行为数据集的空白。（2）本文提出了一种改进模型，命名为YOLOv7+BRA。我们在模型中添加了一个双层路由注意模块，使其具有动态查询感知稀疏性。实验结果表明，该方法成功地提高了检测准确率，降低了误检率。（3）该文提出一种改进的模型YOLOv7+Wise IoU，该模型集成了Wise-IoU损失函数，以提高模型在数据质量不均匀的数据集上的性能。通过根据锚箱的质量动态分配梯度增益，Wise-IoU 减少了所需的训练迭代次数，并在 SCB-Dataset 上实现了比现有损失函数更高的准确性。实验结果表明，Wise-IoU v1、v2 和 v3 的性能均优于现有的损失函数，其中 Wise-IoU v3 的综合性能最佳。


引言：
行为检测技术
随着行为检测技术的发展，智慧教室已慢慢的普及到各个高校，从小学到大学无不例外[1]。现阶段智慧教室对于学生课堂行为检测的观察和记录主要是依靠与网络摄像头的监控，通过网络摄像头进行拍照或者录像，也就是说通过人为进行对学生课堂行为的评价，来了解学生的学习情况。因此利用计算机视觉为辅助通过行为检测技术来准确检验和分析学生的行为，为教学提供反馈，从而提高教学的质量。Fan Yang等人在基于视频动作识别、基于姿态估计、基于物体检测的三种行为检测算法中，提出基于对象检测的方法，例如采用YOLOv7来分析学生的行为，然后提出一种改进模型YOLOv7+BRA方法。旨在为了在解决学生场景密集或学生之间遮挡的场景。但是仅仅是提升模型精度是不够的，模型轻量化方向也是一个关键的研究领域，为了能够在具有有限的计算能力和存储空间上的边缘设备中将模型部署到智慧教室，从而达到实时检测和反馈的效果。但是模型轻量化也存在如何将计算量降低的同时，保证或者提高模型准确率，需要一个具有高准确度的轻量级模型，而YOLOv8速度快，精度高、参数低，也是比较优秀稳定的目标检测器之一，同时考虑到数据集存在学生场景密度密集或学生之间存在互相遮挡的领域。
因此，本文旨在通过轻量化YOLOv8模型，同时保持其模型的提取多尺度特征能力和精度下，将模型能够部署到边缘设备上，进而提高教学质量。为了提高YOLOv8的模型，我们对模型采用了几项改进的方法。本文的主要贡献如下：
1.将YOLOv8中C2f模块中Bottleneck部分引入一种新型的卷积PConv代替以往的Conv部分，旨在能够减少在C2f模块在整个网络的运算量，同时在Bottleneck中引入多尺度关注模块EMA，加强对模型的感知能力，从而构建出C2f_PConv_EMA模块
2.引入一种轻量化特征融合网络BiFPN，将原先的PANet网络替换，旨在能够加强网络的多尺度融合能力的同时减少模型权重大小。
3.深入研究了模块的在网络的空间位置之间的影响，以及特征融合网络PAnet和BiFPN与C2f_PConv_EMA模块之间的影响，证明我们在设计网络的时候的可靠性和有效性。


In the backbone network, using C2f\_PConv\_EMA and in the head network using BiFPN with C2f\_PConv\_EMA is denoted as V1;
In the backbone network, using C2f\_PConv\_EMA and in the head network using PANet without C2f\_PConv\_EMA is denoted as V2;
In the backbone network, using C2f\_PConv\_EMA and in the head network using BiFPN without C2f\_PConv\_EMA is denoted as V3;
In the backbone network, using C2f\_PConv\_EMA and in the head network using PANet with C2f\_PConv\_EMA is denoted as V4;
In the backbone network, not using C2f\_PConv\_EMA and in the head network using BiFPN with C2f\_PConv\_EMA is denoted as V5.


我们采用C2f\_PConv\_EMA来替换在backbone网络中所有的C2f模块，同时将头部网络采用BiFPN+C2f\_PConv\_EMA的组合替换原先网络结构PANet+C2f的组合，上述的网络设计称之为V1；

对于V2来说，我们仅仅采用C2f\_PConv\_EMA来替换在backbone网络中所有的C2f模块，但是并不改变原先头部网络结构组合；

对于V3来说，我们还是将所有的backbone网络中的C2f模块换成C2f\_PConv\_EMA，而对于头部网络仅仅采用BiFPN代替原先的PANet，保留原先的C2f模块；

对于V4来说，我们依旧将所有的backbone网络中的C2f模块换成C2f\_PConv\_EMA，而对于头部网络采用PANet+C2f\_PConv\_EMA的组合。

对于V5来说，不修改原先backbone网络组合，仅仅是在头部网络采用BiFPN+C2f\_PConv\_EMA的组合。